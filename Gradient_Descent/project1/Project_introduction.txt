
def find_best_degree(x, y, max_degree=10, learning_rate=0.01, iterations=1000, validation_split=0.2):
    """
    최적의 다항식 차수 찾기 (Train/Validation Split 사용)
    
    Args:
        validation_split: 검증 데이터 비율
    
    Returns:
        best_degree: 최적 차수
        all_results: {degree: (final_coeffs, train_loss, val_loss)}
    """
    # 데이터 분할
    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=validation_split, random_state=42)
    
    all_results = {}
    
    for degree in range(1, max_degree + 1):
        # 학습 데이터로 학습
        history = gradient_descent(x_train, y_train, degree, learning_rate, iterations)
        final_iter, final_coeffs, train_loss = history[-1]
        
        # 검증 데이터로 평가
        y_val_pred = polynomial_predict(x_val, final_coeffs)
        val_loss = mse_loss(y_val, y_val_pred)
        
        all_results[degree] = (final_coeffs, train_loss, val_loss)
        
        print(f"Degree {degree}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}")
    
    # 검증 손실이 가장 낮은 차수 선택
    best_degree = min(all_results.keys(), key=lambda d: all_results[d][2])
    
    return best_degree, all_results